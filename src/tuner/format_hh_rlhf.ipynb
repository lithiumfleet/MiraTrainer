{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dpo/test.jsonl\") as f:\n",
    "    content = '['+','.join(f.readlines())+']'\n",
    "    dataset = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转成{\"prompt\",\"chosen\",\"rejected\"}三元组, 并且给prompt加上模板和assistant:, 给其他两个字段加上<eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data/lixubin/models/Qwen/Qwen1.5-1.8B-Chat/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nWhat are situations I should have an emergency fund for?<|im_end|>\\n<|im_start|>assistant\\nOoh, that’s a big question.  Let’s see if I can put together a good answer.  I know a few ideas.  Here’s one.  I know that people have emergency situations when they have to cover big unexpected expenses, or when they lose their job.  What do you think of that as a possible situation?<|im_end|>\\n<|im_start|>user\\nOkay, so losing job is one of them, yes<|im_end|>\\n<|im_start|>assistant\\nAnother one might be having a bad medical emergency.<|im_end|>\\n<|im_start|>user\\nOkay, yes. So far we have two ideas.<|im_end|>\\n<|im_start|>assistant\\nOne more thing I was thinking of is an unexpected home repair or emergency car repair.<|im_end|>\\n<|im_start|>user\\nOkay, those are good, too. So far we have four ideas. I can't think of any besides those four. How about you?<|im_end|>\\n<|im_start|>assistant\\nHow about emergency travel expenses?<|im_end|>\\n<|im_start|>user\\nThat's a good one!<|im_end|>\\n<|im_start|>assistant\\nHow about other types of unexpected emergencies? Do you have any ideas for others?<|im_end|>\\n<|im_start|>user\\nI can't think of anything else.<|im_end|>\\n<|im_start|>assistant\\n\",\n",
       " 'chosen': \"That's okay.  As long as we have four or more we're good, and we can always add more if we think of some more.<|im_end|>\",\n",
       " 'rejected': 'How about unexpected child expenses? Do you want to add that?<|im_end|>'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(example=dataset[7]):\n",
    "    result = example['chosen'].split(\"\\n\\nAssistant: \")\n",
    "    history = []\n",
    "    for seg in result[:-1]:\n",
    "        history += seg.split(\"\\n\\nHuman: \")\n",
    "    history.pop(0)\n",
    "    messages = []\n",
    "    for index, line in enumerate(history):\n",
    "        if index % 2 == 0:\n",
    "            messages.append({\"role\":\"user\", \"content\":line})\n",
    "        else:\n",
    "            messages.append({\"role\":\"assistant\", \"content\":line})\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    chosen = result[-1]+tokenizer.eos_token\n",
    "    rejected = example['rejected'].split(\"\\n\\nAssistant: \")[-1]+tokenizer.eos_token\n",
    "\n",
    "    # print(prompt)\n",
    "    # print(chosen)\n",
    "    # print(rejected)\n",
    "    # print(\"=\"*20)\n",
    "    # print(example['chosen'])\n",
    "    return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for example in dataset:\n",
    "    new_dataset.append(process(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/lixubin/MiraTrainer/src/data/dpo/new_test.json\", 'w') as f:\n",
    "    json.dump(new_dataset, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
